9️⃣ 종합 진단 보고서 생성...
2025-08-12 09:21:20,834 - INFO -
=== 📋 종합 진단 보고서 생성 ===
2025-08-12 09:21:20,836 - INFO - 📄 진단 보고서 저장: ollama_diagnosis_20250812_092120.txt
2025-08-12 09:21:20,837 - INFO -
Ollama LLM 종합 진단 보고서
==========================
진단 일시: 2025-08-12 09:21:20
시스템: Windows 11

🔍 진단 결과 요약
-----------------
✅ Ollama 설치: ollama version is 0.11.4
❌ Ollama 서버: 실행되지 않음
❌ 모델 상태: 설치된 모델 없음
❌ Python 통합: 문제 있음

💾 시스템 리소스
  - CPU: 22코어
  - 메모리: 31.5GB 총용량
  - 사용 가능 메모리: 9.59GB
  - 디스크 여유공간: 405.98GB

💡 권장사항
  2. Ollama 서버 시작: 터미널에서 'ollama serve' 실행
  3. 권장 모델 설치:
     - ollama pull qwen2.5:7b  (한국어 최적화)
     - ollama pull llama3.2:3b (경량 모델)

📄 보고서 파일: ollama_diagnosis_20250812_092120.txt

2025-08-12 09:21:20,840 - INFO - 😟 여러 문제가 발견되었습니다. 권장사항을 따라 문제를 해결해주세요.

==================================================
🏁 진단 완료 - 전체 상태: poor

D:\dev_proj\outlook-free-analyzer>python diagnose_llm.py    

D:\dev_proj\outlook-free-analyzer>ollama ps
NAME                             ID              SIZE      PROCESSOR          CONTEXT    UNTIL
coolsoon/kanana-1.5-8b:latest    dd34f3a73579    6.5 GB    18%/82% CPU/GPU    4096       4 minutes from now

time=2025-08-12T09:20:42.974+09:00 level=INFO source=server.go:438 msg="starting llama server" cmd="C:\\Users\\yoursoul.koo\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\yoursoul.koo\\.ollama\\models\\blobs\\sha256-887825d277f138e0c6069a02bb1a264142629c61c570935e4562210f691e3f6e --ctx-size 4096 --batch-size 512 --n-gpu-layers 27 --threads 6 --no-mmap --parallel 1 --port 56742"
time=2025-08-12T09:20:43.020+09:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-08-12T09:20:43.020+09:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-08-12T09:20:43.022+09:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T09:20:43.363+09:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4050 Laptop GPU, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from C:\Users\yoursoul.koo\AppData\Local\Programs\Ollama\lib\ollama\ggml-cuda.dll
load_backend: loaded CPU backend from C:\Users\yoursoul.koo\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
time=2025-08-12T09:20:43.716+09:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T09:20:43.723+09:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:56742"
time=2025-08-12T09:20:43.778+09:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4050 Laptop GPU) - 5091 MiB free
llama_model_loader: loaded meta data with 33 key-value pairs and 291 tensors from C:\Users\yoursoul.koo\.ollama\models\blobs\sha256-887825d277f138e0c6069a02bb1a264142629c61c570935e4562210f691e3f6e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Kanana 1.5 8b Instruct 2505
llama_model_loader: - kv   3:                            general.version str              = 2505
llama_model_loader: - kv   4:                           general.finetune str              = instruct
llama_model_loader: - kv   5:                           general.basename str              = kanana-1.5
llama_model_loader: - kv   6:                         general.size_label str              = 8B
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                               general.tags arr[str,1]       = ["text-generation"]
llama_model_loader: - kv   9:                          general.languages arr[str,2]       = ["en", "ko"]
llama_model_loader: - kv  10:                          llama.block_count u32              = 32
llama_model_loader: - kv  11:                       llama.context_length u32              = 32768
llama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  16:                       llama.rope.freq_base f32              = 8000000.000000
llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  18:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  19:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128259
llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128259]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128259]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 128001
llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {# version=v3-llama3.1 #}{%- macro ap...
llama_model_loader: - kv  31:               general.quantization_version u32              = 2
llama_model_loader: - kv  32:                          general.file_type u32              = 15
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW)
load: special tokens cache size = 259
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 8000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Kanana 1.5 8b Instruct 2505
print_info: vocab type       = BPE
print_info: n_vocab          = 128259
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128256 '<|eom_id|>'
print_info: PAD token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: EOG token        = 128256 '<|eom_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
[GIN] 2025/08/12 - 09:20:46 | 200 |       546.8µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 09:20:46 | 200 |            0s |       127.0.0.1 | GET      "/api/ps"
load_tensors: offloading 27 repeating layers to GPU
load_tensors: offloaded 27/33 layers to GPU
load_tensors:    CUDA_Host model buffer size =  1058.03 MiB
load_tensors:        CUDA0 model buffer size =  3345.47 MiB
load_tensors:          CPU model buffer size =   281.82 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 8000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   432.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    80.00 MiB
llama_kv_cache_unified: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      CUDA0 compute buffer size =   669.49 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1094
llama_context: graph splits = 59 (with bs=512), 3 (with bs=1)
time=2025-08-12T09:20:49.554+09:00 level=INFO source=server.go:637 msg="llama runner started in 6.53 seconds"
[GIN] 2025/08/12 - 09:20:51 | 200 |    9.2944008s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/12 - 09:21:13 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/08/12 - 09:21:42 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 09:21:42 | 200 |            0s |       127.0.0.1 | GET      "/api/ps"

